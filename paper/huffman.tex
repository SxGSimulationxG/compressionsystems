\subsection{Huffman Coding}
In Huffman's system, we start with a sequence code $\mathcal{S}$ that is sorted by descending probability and assign each of these characters a node in $\mathcal{Q}$.
\begin{equation}
	p_i\geq p_{i+1}\geq \dots \geq p_m
\end{equation}
\begin{equation}
	q_i \leftarrow s_i, (\tau_i= p_i)
\end{equation}

We then take a maximum of $r$ but at least 2 characters with the lowest probability and connect them together with a new node and put this new node back in the array $\mathcal{Q}$ in the right position corresponding to its weight.
\begin{equation}
	q_{new}\leftarrow {q_{new}}_j, \tau_{new}=\sum^j_{i=1}\tau_i
\end{equation}

You can repeat this process until $\tau_{new}=1$. At the end, we can visualise the compression tree by tracing out the children of each node until we reach the source characters. \footnote{\cite{huffmanOriginal} p.1011: Generalization of the Method; \cite{rueda2002advances} p.50: Algorithm 1 Static Huffman Encoding}

\subsubsection{Example: Static Huffman Coding}

Input:
\begin{itemize}
	\item $\mathcal{S}=\{a,b,c,d,e,f,g,h\}$
	\item $\mathcal{P}=\{0.22,0.20,0.18,0.15,0.10,0.08,0.05,0.02\}$
	\item $\mathcal{A}=\{0,1,2,3\}$
\end{itemize}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		$s_i$ & $p_i$ & $L_i$ & Code \\
		a & 0.22 & 1 & 1 \\
		b & 0.20 & 1 & 2 \\
		c & 0.18 & 1 & 3 \\
		d & 0.15 & 2 & 00 \\
		e & 0.10 & 2 & 01 \\
		f & 0.08 & 2 & 02 \\
		g & 0.05 & 3 & 030 \\
		h & 0.02 & 3 & 031 \\
	\end{tabular}
	\caption{Adapted from \cite{huffmanOriginal} p.1101: Table III}
	\label{tab:placeholder}
\end{table}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.1\linewidth]{huffmanTree}
	\caption{Huffman Tree of Table 1; Made using Microsoft PowerPoint}
	\label{fig:huffmantree}
\end{figure}

In Figure 1, the green nodes represent the single characters with their corresponding probabilities $p$ and the purple nodes are the connecting nodes $q$ with their corresponding weight $\tau$. While encoding and decoding, we use this tree as a translation guide, the furthest left being 0, then 1, and so on.


\subsubsection{Adaptive Huffman Coding}
Adaptive Huffman Coding or Dynamic Huffman Coding, in comparison to the static version, is a system that adapts the construction of the Huffman tree to changes in the count of characters, rather than having a presorted list with given probabilities. This theory suggested by Robert G. Gallager, is based on the sibling property, where if the nodes can be listed in decreasing counts and each node is adjacent to its sibling, then the code is optimal for the current character count\footnote{\cite{gallager2003variations} p.6: 2. The Sibling Property Definition}. In a binary system each node, or sibling pair, contains 5 different components. Two are the current counters of the children nodes, two are links to the children nodes, and the last is a link to parent node. Each of the links also having an extra property indicating whether the node is the 0th or the 1st child node. As an input to our adaptive system, we have the Huffman tree of one less character read and the next character. The output then being the Huffman tree with the updated count. With the incrementing of the character count, if it is found that the count exceeds the count of the next higher sibling pair, then these two nodes are interchanged, changing the code and the resulting Huffman tree\footnote{\cite{gallager2003variations} p.17-21: Adaptive Huffman Codes}. This algorithm was then further expanded upon by Donald E. Knuth with his paper on Dynamic Huffman Coding. Here he fills in some gaps left by Gallager's adaptive system like the decrementing of the character counts\footnote{\cite{knuthDynamicHuffman} p.163-165: Introduction}.