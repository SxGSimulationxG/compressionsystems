\section{Conclusion}
In general, even though the original compression systems of Huffman, Shannon, and Fano are not used that frequently any more in their original forms, they lay the foundation of modern lossless compression systems. Huffman shows us that in a zeroth order compression system, an optimal compression algorithm exists and functions using a bottom-up construction system. Fano and Shannon coding system on the other hand might not be as effective when it comes to getting close to the value of the entropy, their coding systems can give a more intuitive overview and way of implementing a lossless compression algorithm. Looking at the examples we used we can see though, that the difference between their compression sizes is quite small at first but increases the larger the file size gets. In the example we can also see Shannon's theory work, where the larger the file size gets the more storage is saved. When we start looking at higher-order models, things can get much more complicated and more storage efficient, but it almost always ends up with a zeroth-order compression system just that the input can be more effectively compressed than the original input. 

