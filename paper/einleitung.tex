\section{Einleitung}
\textbf{Abstrakt - Der Vergleich zwischen den ursprünglichen Datenkompressionsalgorithmen, den Huffman- und Shannon-Fano-Kodierungssystemen, sowie eine Betrachtung der verschiedenen Verbesserungen und Anwendungen verlustfreier Kompressionssalgorithmen.}\\

Verlustfreie Datenkompressionssysteme sind für unser modernes digitales Leben von zentraler Bedeutung. Ohne sie wäre der heutige technologische Fortschritt kaum denkbar. Wie der Name schon andeutet, handelt es sich dabei um Verfahren, die den Speicherbedarf von Daten reduzieren, ohne dabei Informationen zu verlieren. Die Grundlage dieser Systeme bilden die Kodierungsverfahren von Claude E. Shannon, Robert Fano und David A. Huffman. Seit ihrer Einführung wurden zahlreiche Varianten und Verbesserungen entwickelt, die unterschiedliche Ansätze zur Optimierung der Kompressionsleistung verfolgen. Dieses Prinzip ist eng mit der Geschichte der Kryptologie verbunden. Bereits im 4. Jahrhundert v. Chr. können wir in Griechenland beobachten, wie Polybius von einem verbesserten Kommunikationssystem berichtet, das von Aeneas Tacticus eingeführt wurde. Dabei verwendeten zwei Personen, die sich in Sichtweite befanden, eine Fackel und zwei identische, beschriftete Gefäße, um in Kriegszeiten schneller zu kommunizieren\footnote{\cite{polybiusHistories} Book X S.42f.: 44. The Improvement introduced by Aeneas Tacticus}. Hier verwendeten sie Wasser und Gravuren in den Gefäßen, wobei eine bestimmte Wassermenge eine bestimmte Nachricht darstellte. Dies ist ein frühes Beispiel für eine kryptographische Chiffre, bei der eine bestimmte Eingabe, hier die Wassermenge, zu einer bestimmten Klartextnachricht entschlüsselt werden kann\footnote{\cite{cnssGlossary} S.32: Cipher (American English)}. Spätere optische Telegraphiesysteme (Semaphoren) folgten einem ähnlichen Prinzip: vordefinierte Zeichenstellungen standen für festgelegte Botschaften. Der Nachteil solcher Systeme lag in der begrenzten Informationsmenge und der zeitaufwendigen Übermittlung einzelner Zeichen. Daraus entstand die Idee, Nachrichten möglichst effizient zu kodieren, also Daten zu komprimieren, ohne Informationen zu verlieren. Den ersten theoretisch fundierten Ansatz entwickelte Claude Shannon in seinem „noiseless coding theorem“, das die Grundlage der modernen Informationstheorie bildet\footnote{\cite{rueda2002advances} S.36: 2.2.1 Introduction to Information Theory}. Etwa zur gleichen Zeit entwickelte Robert Fano das später als Fano-Kodierung bekannte Verfahren. Obwohl sich die Kodierungssysteme von Shannon und Fano grundlegend unterscheiden, werden sie oft zur Shannon-Fano-Codierung kombiniert, wobei meist das Kodierungssystem von Fano gemeint ist. Beide Systeme sind jedoch suboptimal, wenn es um die resultierende Kompressionsgröße geht\footnote{\cite{krajvci2015performance} S.1: Introduction}. Eine entscheidende Verbesserung gelang David A. Huffman im Rahmen eines Seminars zur Informationstheorie bei Robert Fano. Anstatt eine Abschlussprüfung abzulegen, entschied er sich für eine Projektarbeit mit dem Ziel, das optimale verlustfreie Kompressionsverfahren für ein binäres System zu finden – eine Aufgabe, an der sich selbst Shannon und Fano versucht hatten. Das Ergebnis war einer der einflussreichsten Arbeiten auf diesem Gebiet: „A method for the construction of minimum-redundancy codes” und bewies damit, dass dies das optimale System zur Konstruktion eines solchen verlustfreien Komprimierungssystems war\footnote{\cite{pivkinadiscovery}: S.1: Introduction}. Seitdem wurde das Feld der verlustfreien Datenkompression erheblich weiterentwickelt. Moderne Verfahren wie adaptive, kontextbasierte oder grammatikbasierte Systeme bauen direkt auf den ursprünglichen Ideen von Shannon, Fano und Huffman auf und passen sie an neue Anwendungen und Datenstrukturen an. Die Bedeutung der jeweiligen Verbesserungen hängt jedoch stark vom Anwendungsbereich ab, weshalb eine vollständige Darstellung aller Varianten im Rahmen dieser Arbeit nicht möglich ist.

\subsection{Zielsetzung}

Diese Seminararbeit erläutert zunächst die Funktionsweise der grundlegenden Kompressionssysteme von Huffman, Shannon und Fano und zeigt, worin sich diese Verfahren unterscheiden. Anschließend werden die Unterschiede zwischen statischen und adaptiven Systemen beschrieben sowie deren Weiterentwicklungen und heutige Einsatzgebiete untersucht. Schließlich werden die drei Basissysteme mithilfe eines in Java implementierten Vergleichssystems anhand von Beispieldaten praktisch gegenübergestellt, um die Unterschiede in ihrer Effizienz zu analysieren und mögliche Auffälligkeiten in den Ergebnissen zu identifizieren.

\subsection{Grundlegende Prinzipien}

\subsubsection{Unterschied zwischen verlustfreie und verlustbehaftete Kompressionssysteme}
Verlustfreie Kompressionssysteme sind Verfahren, bei denen die ursprünglichen Daten nach der Dekompression vollständig und in unveränderter Qualität wiederhergestellt werden können.
Im Gegensatz dazu entfernen verlustbehaftete Kompressionssysteme gezielt weniger relevante Informationen, um eine höhere Kompressionsrate zu erzielen\footnote{\cite{blelloch2001introduction} S.40f.: 7 Lossy Compression Techniques}. In dieser Arbeit befassen wir uns ausschließlich mit verlustfreien Kompressionssystemen.

\subsubsection{Informationsentropie}
Die Informationsentropie beschreibt den durchschnittlichen Informationsgehalt pro Symbol in einer Datenquelle. Bei einem optimalen verlustfreien Kompressionssystem entspricht die mittlere Codewortlänge genau dem Wert der Entropie. Dieses Konzept wurde von Claude E. Shannon als zentrales Element seiner Informationstheorie eingeführt. Sinkt die durchschnittliche Codewortlänge unter den Entropiewert $H$, geht Information verloren und das System ist dann nicht mehr verlustfrei.

\begin{equation}
	H = - \sum p_i \log(p_i)
\end{equation}

Diese Gleichung gilt unter der Annahme, dass die Eingabezeichen unabhängig voneinander auftreten\footnote{\cite{shannonMathematicalCommunication} S.396–399: 7 The Entropy of an Information Source}.

\subsubsection{Unterschied zwischen Higher-order und Zeroth-order Systemen}
In einem zeroth-order System wird jedes Zeichen unabhängig voneinander kodiert. Korrelationen zwischen aufeinanderfolgenden Zeichen in der Eingabesequenz werden also nicht berücksichtigt.
Ein higher-order System hingegen erkennt und nutzt Abhängigkeiten über mehrere Zeichen hinweg, wodurch sich die Kompressionseffizienz oft deutlich verbessert\footnote{\cite{rueda2002advances} S.40–44: 2.2.4 Higher-order Sources}.

\subsubsection{Unterschied zwischen statischen und adaptiven Kodierungssystemen}
In einem statischen Kodierungssystem sind die Wahrscheinlichkeiten der Eingabezeichen bereits vor der Kodierung bekannt und bleiben während des gesamten Prozesses unverändert.
Ein adaptives System hingegen kennt diese Wahrscheinlichkeiten nicht im Voraus. Es analysiert die Daten während der Kodierung und passt das Kodierungsschema dynamisch an die Häufigkeit der bisher aufgetretenen Zeichen an\footnote{\cite{rueda2002advances} S.60f.: 2.5 Adaptive Coding}.

\subsubsection{Optimalität}
Die Optimalität eines Codes wird von Huffman als ein Kodierungsschema definiert, das die kürzestmögliche durchschnittliche Codewortlänge bei minimaler Redundanz besitzt (engl. minimum-redundancy codes)\footnote{\cite{huffmanOriginal} S.1098: Introduction}.
Ein theoretisch optimaler Kodierungsalgorithmus erreicht eine mittlere Codewortlänge, die exakt dem Entropiewert $H$ der Eingabesequenz entspricht.

\subsubsection{Präfixfreiheit}
Die Präfixfreiheit (oder Präfixeigenschaft) bedeutet, dass kein Codewort das Präfix eines anderen sein darf. Dadurch kann jedes Codewort eindeutig identifiziert werden, was die Dekodierung erheblich vereinfacht\footnote{\cite{steinruecken2015lossless} S.22f.: 2.1 Symbol Codes}.

Ein Beispiel für ein nicht präfixfreies Kodierungsschema wäre:
$a \rightarrow 0$ und $b \rightarrow 00$.
Hier ist das Codewort von $a$ ein Präfix von $b$, sodass beim Lesen von $00$ unklar bleibt, ob es sich um ein einzelnes $b$ oder zwei aufeinanderfolgende $a$ handelt.