\section{Einleitung}
\textbf{Abstrakt - Der Vergleich zwischen den ursprünglichen Datenkompressionsalgorithmen, den Huffman- und Shannon-Fano-Kodierungssystemen, sowie eine Betrachtung der verschiedenen Verbesserungen und Anwendungen verlustfreier Kompressionssalgorithmen.}\\

Verlustfreie Datenkompressionssysteme sind für unser modernes Leben von enormer Bedeutung. Ohne diese Systeme hätten wir dieses technologische Niveau nicht erreichen können. Wie der Name schon sagt, handelt es sich dabei um Systeme, die den Speicherplatzbedarf von Daten reduzieren, ohne dabei Informationen zu verlieren. Die Grundlage verlustfreier Datenkomprimierungssysteme bilden die Codierungssysteme von Huffman, Shannon und Fano. Seit der Einführung dieser Systeme gab es jedoch zahlreiche Verbesserungen und unterschiedliche Ansätze für das Problem der verlustfreien Datenkomprimierung. Dieses Prinzip ist eng mit der Geschichte der Kryptologie verbunden. Bereits im 4. Jahrhundert v. Chr. können wir in Griechenland beobachten, wie Polybius von einem verbesserten Kommunikationssystem berichtet, das von Aeneas Tacticus eingeführt wurde. Dabei verwendeten zwei Personen, die sich in Sichtweite befanden, eine Fackel und zwei identische, beschriftete Gefäße, um in Kriegszeiten schneller zu kommunizieren\footnote{\cite{polybiusHistories} Book X pp.42-43: 44. The Improvement introduced by Aeneas Tacticus}. Hier verwendeten sie Wasser und Gravuren in den Gefäßen, wobei eine bestimmte Wassermenge eine bestimmte Nachricht darstellte. Dies ist ein frühes Beispiel für eine kryptographische Chiffre, bei der eine bestimmte Eingabe, hier die Wassermenge, zu einer bestimmten Klartextnachricht entschlüsselt werden kann\footnote{\cite{cnssGlossary} p.32: Cipher (American English)}. In ähnlicher Weise wurden viele Versionen des optischen Semaphors verwendet, wobei vordefinierte Interpretationen bestimmter Positionen verwendet wurden, um eine vorab festgelegte Nachricht zu übermitteln. Einer der Nachteile beider Systeme besteht darin, dass die übertragbaren Informationen begrenzt sind, da nicht jede mögliche Nachricht in eine Chiffre passt. Die Übermittlung einzelner Buchstaben würde sehr viel Zeit in Anspruch nehmen. Um die zu sendende Datenmenge zu reduzieren und gleichzeitig alle für die Erstellung einer Nachricht erforderlichen Informationen zu erhalten, entstand die Idee der verlustfreien Datenkompression. Der erste wichtige Kompressionsalgorithmus war die nach ihm benannte Kodierungssystem von Claude Shannon, das auf seinem Theorem zur rauschfreien Kodierung basiert (Englisch: noiseless coding theorem). Diese Theorem und Kompressionsalgorithmus bilden die Grundlage dessen, was später zum Gebiet der Informationstheorie werden sollte\footnote{\cite{rueda2002advances} p.36: 2.2.1 Introduction to Information Theory}. Etwa zur gleichen Zeit entwickelte Robert Fano das später als Fano-Kodierung bekannte Verfahren. Obwohl sich die Codierungssysteme von Shannon und Fano grundlegend unterscheiden, auch wenn sie in etwa auf die gleiche Weise funktionieren, werden sie oft zur Shannon-Fano-Codierung kombiniert, wobei meist das Codierungssystem von Fano gemeint ist. Beide Systeme sind jedoch suboptimal, wenn es um die resultierende Kompressionsgröße geht\footnote{\cite{krajvci2015performance} p.1: Introduction}. Das änderte sich, als David A. Huffman einen Elektrotechnik-Aufbaustudiengang zum Thema Informationstheorie bei Robert Fano besuchte. In diesem Kurs hatten die Studenten die Wahl zwischen einer Semesterarbeit und einer Abschlussprüfung. Das Thema der Semesterarbeit bestand darin, das optimale verlustfreie Komprimierungssystem für ein binäres System zu finden – genau das, was Fano und Shannon selbst zu erreichen versuchten. Huffman entschied sich, die Herausforderung anzunehmen und die Semesterarbeit zu schreiben. Das Ergebnis war eine der einflussreichsten Arbeiten auf diesem Gebiet: „A method for the construction of minimum-redundancy codes”. Er bewies, dass dies das optimale System zur Konstruktion eines solchen verlustfreien Komprimierungssystems war\footnote{\cite{pivkinadiscovery}: p.1: Introduction}. Seitdem gab es viele Verbesserungen auf diesem Gebiet, von denen wir einige im Abschnitt über higher-order Systeme und im Zusammenhang mit den adaptiven Versionen dieser ursprünglichen Systeme diskutieren werden. Diese Verbesserungen sind jedoch sehr unterschiedlich, sodass es unmöglich wäre, sie alle vorzustellen und die Wichtigkeit dieser Verbesserungen hängen stark vom Anwendungsbereich ab. 


\subsection{Zielsetzung}

In der folgenden Seminararbeit werden zunächst die Funktionsweise der grundlegenden Kompressionssysteme Huffman, Shannon und Fano erklärt, und wie sie sich voneinander unterscheiden. Anschließend wird auf den Unterschied zwischen statische und adaptive Systeme eingegangen und wie die gesammten Systeme verbessert wurden und wie sie heutzutage eingesetzt werden. Danach werden die grundlegenden Kompressionssysteme anhand von Beispieldaten mit einem Java-Vergleichssystem verglichen, um herauszufinden, wie stark sich diese Systeme unterscheiden und ob es bestimmte Auffälligkeiten bei den Datenbeispielen gibt. 

\subsection{Grundlegende Prinzipien}

\subsubsection{Unterschied zwischen verlustfreie und verlustbehaftete Kompressionssysteme}
Verlustfreie Kompressionssysteme sind eine Kompressionsmethode, bei der die originale unkomprimierte Form in ihrer originalen Qualität rekonstruiert werden kann. Im Gegensatz dazu stehen die verlustbehafteten Kompressionssysteme, bei dem weniger relevante Informationen verworfen werden, um die Größe zu komprimieren\footnote{\cite{blelloch2001introduction} p.40-41: 7 Lossy Compression Techniques}. Hier befassen wir uns ausschließlich mit verlustfreien Kompressionssystemen. 

\subsubsection{Informationsentropie}
Die Informationsentropie beschreibt die durchschnittliche Informationsgehalt pro Bit. Bei einem optimalen verlustfreien Kompressionssystem entspricht die durchschnittliche Codewortlänge dem Wert der Informationsentropie. Dieses Konzept war eines der wichtigsten in Shannons Grundlagen der Informationstheorie. In dem Moment, in dem die durchschnittliche Codewortlänge unter den Wert der Entropie $H$ fällt, geht Information verloren, sodass das Kompressionssystem nicht mehr verlustfrei ist. 

\begin{equation}
	H=-\sum p_i \log(p_i)
\end{equation}

Diese Gleichung für die Entropie gilt, wenn die Eingabezeichen mit deren Auftrittwahrscheinlichkeit voneinander unabhängig sind\footnote{\cite{shannonMathematicalCommunication} p.396-399: 7 The Entropy of an Information Source}.

\subsubsection{Unterschied zwischen Higher-order und Zeroth-order Systemen}
In einem zeroth-order System wird jedes Zeichen unabhängig voneinander codiert, wobei Korrelationen zwischen den Zeichen in der Eingabesequenz nicht berücksichtigt werden. Ein higher-order System ist in der Lage, Muster über mehrere Zeichen hinweg zu erkennen und zu nutzen\footnote{\cite{rueda2002advances} p.40-44: 2.2.4 Higher-order Sources}. 

\subsubsection{Unterschied zwischen statischen und adaptiven Kodierungssystemen}
In einem statischen Kodierungssystem verfügen wir vor der Kodierung der Sequenz über Kenntnisse hinsichtlich der Wahrscheinlichkeiten der Quellsequenz. In einem adaptiven System ist die einzige Eingabe, über die der Kodierer verfügt, die Quellsequenz selbst. Das System passt dann das Kodierungsschema an die Änderungen in der Häufigkeit der Eingabezeichen an\footnote{\cite{rueda2002advances} p.60-61: 2.5 Adaptive Coding}.

\subsubsection{Optimalität}
Die Optimalität eines Codes wird von Huffman beschreiben als eine Kodierungsschema, welche die kurzmöglichste durschnittliche Codewortlänge besitzt und so am wenigsten Redundanz besitzt (engl. minimum-redundancy codes)\footnote{\cite{huffmanOriginal} p.1098: Introduction}. Diese kurzmöglichste Codewortlänge wird auch in der Prinzip der Informationsentropie von Shannon beschrieben. Die theoretisch optimale Kodierungsalgorithmus besitzt eine durchschnittliche Codewortlänge, welche gleich ist wie die Wert der Informationsentropie eines Eingabesequenz besitzt. 

\subsubsection{Präfixfreiheit}
Die Präfixfreiheit oder Präfixeigenschaft besagt, dass ein Codewort kein Präfix einer anderen sein kann. So kann kein Codewort mit einem anderen verwechselt werden und die Ende eines Codeworts eindeutig gefunden werden. Dies macht die Dekodierung deutlich einfacher\footnote{\cite{steinruecken2015lossless} p.22-23: 2.1 Symbol Codes}. Ein Beispiel für einer Kodierungsschema welche diese Eigenschaft nicht besitzt, wäre wenn man zum Beispiel $a$ mit $0$ kodiert und $b$ mit $00$. $a$ ist hier ein Präfix von $b$. So kann man bei der Codewort $00$ nicht wissen ob es sich um einen $b$ handelt oder zwei $a$s.