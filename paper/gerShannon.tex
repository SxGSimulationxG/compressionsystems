\subsection{Shannon Codierung}
Auch wenn die von Claude E. Shannon vorgeschlagene Shannon-Methode aufgrund ihrer Ineffizienz hinsichtlich der resultierenden Kompressionsgröße nicht häufig verwendet wird, hat sie dennoch eine gewisse Relevanz in diesem Bereich.
Dieses Verfahren ordnet jedem Symbol über eine kumulative Wahrscheinlichkeitsfunktion $P_i$ einen eindeutigen Abschnitt im Intervall $[0;1[$ zu und erzeugt daraus eine Zahlenfolge, die als Grundlage für die Codewörter dient\footnote{\cite{rueda2002advances} S.52f.: 2.4.2 Shannon's Method; \cite{shannonMathematicalCommunication} S.401-403: 9. The Fundamental Theorem for a Noiseless Channel}. Die kumulative Wahrscheinlichkeitsfunktion wird rekursiv definiert als:

\begin{equation}
	P_i = \left\{
	\begin{array}{lll}
		0 & \textbf{for} & i = 1 \\
		P_{i-1} + p_{i-1} & \textbf{for} & 2 \leq i \leq m
	\end{array}
	\right.
\end{equation}

Die Länge jedes Codeworts ergibt sich aus der Wahrscheinlichkeit des jeweiligen Symbols nach:

\begin{equation}
	l_i=\lceil log_r(p_i^{-1})\rceil
\end{equation}

wobei seltener auftretende Symbole längere Codewörter erhalten.\\

Shannon stellt außerdem fest, dass mit zunehmender Länge $N$, sich die durchschnittliche Codewortlänge $H'$ der Entropie $H$ nähert, was zeigt, dass Shannons Methode zwar nicht optimal, aber asymptotisch effizient ist.

\begin{equation}
	H'=\frac{1}{N} \sum(m_s p_s)
\end{equation}

Hier ist $p_s$ der Wert für die kumulative Wahrscheinlichkeitsfunktion $P_i$. $m_s$ ist eine ganze Zahl, wo:
\begin{equation}
	\log_2(\frac{1}{p_s}) \leq m_s < \log_2(\frac{1}{p_s}) + 1
\end{equation}

\begin{equation}
	H \leq H' < H + \frac{1}{N}
\end{equation}

Damit liefert Shannon zwar kein optimales, aber ein einfach berechenbares Codierungssystem. Shannon beschreibt auch die Grenzen der theoretischen Effizienz und bildet die Grundlagen für spätere Verfahren.

\subsubsection{Beispiel: Statisches Shannon Kodierung}

Eingabe:
\begin{itemize}
	\item $\mathcal{S}=\{a,b,c,d,e,f,g,h\}$
	\item $\mathcal{P}=\{0.22,0.20,0.18,0.15,0.10,0.08,0.05,0.02\}$
	\item $\mathcal{A}=\{0,1\} \to r=2$
\end{itemize}

\begin{table}[ht]
	\centering
	\begin{tabular}{c|c|c|c|c|c}
		$s_i$ & $p_i$ & $P_i$ & $l_i$ & $r$-Darstellung von $P_i$ & Code \\
		a & 0.22 & 0 & 3 & $0.000\dots$ & 000 \\
		b & 0.20 & 0.22 & 3 & $0.001\dots$ & 001 \\
		c & 0.18 & 0.42 & 3 & $0.011\dots$ & 011 \\
		d & 0.15 & 0.60 & 3 & $0.100\dots$ & 100 \\
		e & 0.10 & 0.75 & 4 & $0.11$ & 1100 \\
		f & 0.08 & 0.85 & 4 & $0.1101\dots$ & 1101 \\
		g & 0.05 & 0.93 & 5 & $0.11101\dots$ & 11101 \\
		h & 0.02 & 0.98 & 6 & $0.111110\dots$ & 111110 \\
	\end{tabular}
	\caption{Angepasst aus Beispiel von \cite{huffmanOriginal} S.1101: Table III}
	\label{tab:placeholder}
\end{table}

\subsubsection{Adaptive Shannon Kodierung}

Travis Gagie schlägt eine Methode zur Erstellung eines adaptiven Shannon-Kodierungssystems vor. Bei dieser Methode werden die Aufgaben in Vordergrund- und Hintergrundprozesse unterteilt. Die Vordergrundaufgabe berücksichtigt das Gewicht der vorherigen Zeichenanzahl und aktualisiert die Gewichte entsprechend dem neu eingegebenen Zeichen. Befindet sich das Zeichen noch nicht im Kodierungsschema, wird der Knoten zur Zeichenanzahl hinzugefügt. Im Hintergrund wird parallel für jedes im Vordergrund verarbeitete Zeichen ein Zeichen nachgeführt, sodass die Gewichtungen kontinuierlich an die tatsächliche Symbolverteilung angepasst werden. Die Gewichtung eines bestimmten Zeichens $a$ wird also jedes Mal aktualisiert, wenn es auftritt, oder neu mit einem Gewicht eingefügt, das seiner geschätzten Wahrscheinlichkeit entspricht. Diese Trennung ermöglicht es, die Wahrscheinlichkeitsverteilung laufend zu aktualisieren, ohne den gesamten Kodierungsbaum neu zu berechnen\footnote{\cite{gagie2004dynamic} S.3f.: III Dynamic Shannon Coding}. 