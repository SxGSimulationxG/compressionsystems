\section{History of Data Compression}
In general, a data compression system is used when we want to compact a larger amount of data into a smaller amount of information and as a result also decrease the amount of time required to transfer messages. This principle can already be seen in the 4th century BCE in Greece where Polybius tells us of an improved communication system introduced by Aeneas Tacticus, in which two people in viewing distance would use a torch and two identical inscribed vessels to communicate at faster speeds in times of war\footnote{\cite{polybiusHistories} Book X pp.42-43: 44. The Improvement introduced by Aeneas Tacticus}. Here they used water and engravings in the vessel, a certain volume of water being a specific message. This is an early example of a cryptographic cypher, where a certain input, here the volume of water, can be decrypted to a certain plain text message\footnote{\cite{cnssGlossary} p.32: Cipher (American English)}. In the same vane, many versions of the optical semaphore were used in a similar manner, using predefined interpretations of certain positions to communicate a predetermined message. However, one of the flaws of both of these systems is that there is a limit to what information you can transmit as you cannot fit every possible message into a cypher. Communicating a single letter at a time would take a significant amount of time so to reduce the quantity of data being sent but retain all the information required to produce any message, the idea of lossless data compression was born. The first main compression algorithm to come up was Claude Shannon's coding system, named after himself (See Section 3.2: Shannon Coding) based on his noiseless coding theorem. This theorem and compression algorithm make the foundations of what would become the field of information theory\footnote{\cite{rueda2002advances} p.36: 2.2.1 Introduction to Information Theory}. 
At roughly the same time Robert Fano created what would become known as Fano coding. Even though Shannon's and Fano's coding systems are fundamentally different even if they do function in roughly the same manner, they are often combined into Shannon-Fano coding, mostly referring to Fano's coding system. These systems however are both suboptimal when it comes to the resulting compression size\footnote{\cite{krajvci2015performance} p.1: Introduction}. 
It all then changed when David A. Huffman attended an electrical engineering graduate course on information theory taught by Robert Fano. In this course the students were given the choice between a term paper or a final exam. The topic of the term paper was to find the most optimal lossless compression system for a binary system, the very thing that Fano and Shannon were trying to achieve themselves. Huffman decided to take on the challenge and write the term paper, writing one of the most influential papers in the field, "A method for the construction of minimum-redundancy codes" as a result. This he proved was the most optimal system to construct such a lossless compression system\footnote{\cite{pivkinadiscovery}: p.1: Introduction}.
Since then there have been many improvements to the field, some of which we will discuss in the section on higher-order systems and in connection with the adaptive versions of these original systems, however these improvements vary widely and it would be impossible to showcase all of them. 

\todo{Image of graphic of hydraulic telegraph}
\todo{To history of shannon and fano systems}
\todo{To Huffman}
\todo{To modern extensions and versions (Out of scope)}