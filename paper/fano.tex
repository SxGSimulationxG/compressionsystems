\subsection{Fano Coding}
The Fano coding method is, like the Shannon coding method, based on a top-down construction system. The Fano method starts with a sequence code $\mathcal{S}$ that is sorted by descending probability in $\mathcal{P}$ (See Equation 11). In the binary system, we then split this list into two subgroups of equal probability $\mathcal{S}_0$ and $\mathcal{S}_1$, and as a result also $\mathcal{P}_0$ and $\mathcal{P}_1$. Their codes then starting with either 0 or 1. This process is then repeated until the subdivisions are equal to the single characters. Here the code word length is equal to $\log_2 m$ if $m$ is a power of two. If $m$ is not a power of two, the length of the code word is one of the two closest integers to $\log_2 m$.\footnote{\cite{fanoTransmissionInformation} p.5-6: Selection from N Equally Likely Choices; \cite{rueda2002advances} p.55: Algorithm 3 Static Fano Encoding}

\todo{Exact explanation of process}
\todo{Make Example}
\todo{Make example tree using example}


\subsubsection{Adaptive Fano Coding}

The adaptive Fano coding system functions using the same concept as the adaptive Huffman coding system in the sense that the source character weights are not calculated beforehand. Luis Rueda suggests two different ways to make an adaptive Fano coding system, one they call the brute-force method and the other the greedy method. In the brute-force method for each change in the character counter, the system recomputes the corresponding compression tree. This is of course rather inefficient\footnote{\cite{rueda2006fast} p.1659-1660: 2.1 A brute-force method for adaptive Fano coding}. In the greedy method, we start with a list of the input alphabet and initialise the probabilities of each character to be equal. Whenever a character is encoded a specific partitioning procedure is invoked that outputs the code word. Depending on the code alphabet used, different partitioning procedures can be used, for the sake of simplicity, we will just say that this system in some way or another partitions the probabilities optimally but more information into different partitioning procedures can be found in \cite{rueda2006fast}. As a result however when encoding a character in our input sequence we no longer need to update our compression tree for every new input character. Only the changes in weight of a specific character are imputed and any required changes in the current encoding scheme\footnote{\cite{rueda2006fast} p.1660: 2.2 The greedy encoding algorithm}.

\todo{Explain difference to static}
\todo{Make example}