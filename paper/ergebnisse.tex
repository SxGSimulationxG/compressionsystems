\section{Vergleich der drei grundlegenden Kompressionsalgorithmen}
Um die ursprünglichen Kompressionsalgorithmen zu vergleichen wurden zehn verschiedene Beispieldaten in ein Vergleichsprogramm gegeben\footnote{Siehe https://github.com/SxGSimulationxG/compressionsystems}. Aus den Ausgabewerten wurden zwei Abbildungen erstellt:

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{OriginalStorageSizevsAvgCodeWordLength}
	\caption{Mit Google Sheets erstellt}
	\label{fig:plot1}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{EntropyvsAvgCodeWordLength}
	\caption{Mit Google Sheets erstellt}
	\label{fig:plot2}
\end{figure}

In Abbildung 5 lässt sich kein klarer Zusammenhang zwischen dem anfänglichen Speicherbedarf und der endgültigen Kompressionsgröße erkennen. In Abbildung 6 hingegen zeigt sich ein deutlicher Zusammenhang zwischen der Informationsentropie und der Kompressionsgröße beziehungsweise der durchschnittlichen Codewortlänge. In beiden Abbildungen wird deutlich, dass das Shannon-Verfahren deutlich ineffizienter ist als die Verfahren von Huffman und Fano, die wiederum eine sehr ähnliche Effizienz aufweisen. Dabei liegt Huffman meist knapp unter Fano und ist somit etwas effizienter.